# -*- coding: utf-8 -*-
"""Untitled1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1gi6lMSaGx-P1wFET-owk44zJbpL6jkUH
"""

import streamlit as st
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from transformers import AutoTokenizer, AutoModelForSequenceClassification
import numpy as np
from sklearn.metrics import confusion_matrix
from wordcloud import WordCloud
import plotly.express as px
from streamlit_option_menu import option_menu
from transformers import AutoTokenizer, AutoModelForSequenceClassification
import torch
from PIL import Image

# App Layout
st.set_page_config(page_title="Hate Speech Detection", layout="wide")

# Load model and tokenizer
@st.cache_resource
def load_model():
    model = AutoModelForSequenceClassification.from_pretrained("Danny1221Gm12/modelo-hate-speech-binary2")
    tokenizer = AutoTokenizer.from_pretrained("Danny1221Gm12/modelo-hate-speech-binary2")
    return model, tokenizer


model, tokenizer = load_model()

# Load dataset
@st.cache_data
def load_data():
    df = pd.read_csv("labeled_data.csv")
    df['label'] = df['class'].apply(lambda x: 1 if x == 0 else 0)
    return df

df = load_data()

# Prediction function
def predict(text):
    encoded = tokenizer(
        text,
        return_tensors="pt",
        padding=True,
        truncation=True,
        max_length=128
    )

    input_ids = encoded.get("input_ids")
    attention_mask = encoded.get("attention_mask")

    with torch.no_grad():
        outputs = model(input_ids=input_ids, attention_mask=attention_mask)

    # Softmax y clase predicha
    probs = torch.nn.functional.softmax(outputs.logits, dim=1)
    pred_class = torch.argmax(probs, dim=1).item()
    confidence = probs[0][pred_class].item()

    label = "Hate Speech" if pred_class == 1 else "Not Hate Speech"
    return label, confidence

with st.sidebar:
    selected = option_menu("Main Menu", ["Inference", "Dataset Visualization", "Hyperparameter Tuning", "Model Analysis"],
                           icons=['chat', 'bar-chart', 'gear', 'clipboard-data'],
                           menu_icon="cast", default_index=0)

# Page 1 - Inference
if selected == "Inference":
    st.title("Hate Speech Detection")
    st.markdown("""Daniel Garnelo Martinez A00573086""")
    st.markdown("""Welcome to my final project for the "Modeling Learning with Artificial Intelligence" class""")
    st.markdown("For this project I chose to solve the problem of Hate Speech Detection ")
    st.markdown("I'm really exited, as the page is actually running, and the model is well trained" \
    ", however, the project had a lot of complication, particularly with the training of the model and the time " \
    "involved in it, but this will be discused later un the project, please enjoy the app.")
    st.title("Detector")
    st.markdown("First in the list, we have the detector, here you can write any english sentence and it will predict if the sentence is a hate message or not with the probability of being hate.")

    
    text = st.text_area("Enter a sentence:")
    if st.button("Predict"):
        label, confidence = predict(text)
        st.markdown(f"### Prediction: `{label}`")
        st.markdown(f"### Confidence: `{confidence:.2f}`")

# Page 2 - Dataset Visualization
elif selected == "Dataset Visualization":
    st.title("Dataset Visualization")
    st.markdown("First to start the understanding of the problem we have to look at the chosen dataset," \
    " I used a kaggle dataset, containing several tweeter posts with a lot of hate in them, you can find the dataset in this link: https://www.kaggle.com/datasets/mrmorj/hate-speech-and-offensive-language-dataset")
    
    st.subheader("Class Distribution")
    counts = df['label'].value_counts()
    st.bar_chart(counts)
    st.markdown("Firts lets see the distribution of the dataset, we can se an extreme unbalance" \
    " in the dataset, as the class 0, containing all the hate messages, is disproportionally large" \
    " meanwhile the class 1 is smaller, we can also talk a little bit about our model, as the two classes " \
    " indicate we are using a binary classification model, as we need to differentiate between hate and no hate." \
    "The dataset had to be balanced, using and undersampling technique, ass the unbalance was so damatically" \
    " huge the undersampling was the best option as the information wouldnt be altered.")


    st.subheader("Token Length Distribution")
    df['length'] = df['tweet'].astype(str).apply(lambda x: len(x.split()))
    fig = px.histogram(df, x='length', nbins=30, title='Token Length Histogram')
    st.plotly_chart(fig)
    st.markdown("Then we can see the average lenght of the messages in the dataset," \
    " we can see the average lenght of the messages is 10 words. This being a great challenge to the predictions," \
    " as the model could benefit more of longer text so it can evaluate also context, this model cant")


    st.subheader("Word Cloud")
    text = ' '.join(df['tweet'].astype(str))
    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)
    fig, ax = plt.subplots(figsize=(10, 5))
    ax.imshow(wordcloud, interpolation='bilinear')
    ax.axis('off')
    st.pyplot(fig)
    st.markdown("Now we se a word cloud of our dataset, we can see the words that appear the most" \
    " in the messages, this words and slurs will be really important for the classification.")

# Page 3 - Hyperparameter Tuning
elif selected == "Hyperparameter Tuning":
    st.title("Hyperparameter Tuning")
    st.markdown("At first as I created the model the score in it was really bad, reaching an average of" \
    " 50%, so to make its predictions better I had to tune it to the limit, using the hyperparameters, getting " \
    " up to 80%, a good score, I used binary classification to do this.")
    st.markdown("However, the main barrier in the project was the running time, It wpuld take up to" \
    " 42 hours of running time to get the results and the complete training graphs, I managed to do it once" \
    ", however, I had to move and the session ended, deleting the files, I tried doing it again" \
    " but it was to late, so Ill just put the configuration, ignore the other graphs.")
    st.markdown("""
    Parameters:
    - Learning Rate: 2e-5 to 5e-5
    - Batch Size: 16, 32
    - Epochs: 3 (fixed for time constraints)
    - Dropout: 0.1 - 0.3
    
    Best configuration:
    - Learning rate: 3e-5
    - Batch size: 32
    - Dropout: 0.2
    """)
    st.image("optuna_trials_plot.png", caption="Performance over trials", use_container_width=True)

# Page 4 - Model Analysis
elif selected == "Model Analysis":
    st.title("Model Evaluation and Justification")
    st.markdown("To solve the problem, I trained a Transformers-based text classification " \
    "model to detect hate speech in English using the Kaggle dataset explained above. " \
    "I started by exploring and cleaning the data to fully understand the context "
    "and ensure the model didn't learn unnecessary biases. I then used a pre-trained hugging " \
    "faces model (distilbert-base-uncased), trained with PyTorch, I used this method because it is efficient" \
    "and not that coplex, however it also took a lot of time to run, and applied class balancing " \
    "techniques due to the marked disproportion between hateful and non-hateful examples in the " \
    "dataset. I evaluated performance using metrics such as precision, recall, F1 score, and confusion " \
    "matrix, which helped me identify not only how many correct answers the model got but also " \
    "where it failed. I tuned hyperparameters with Optuna to further improve performance without losing" \
    " generalization. I documented the entire process and translated it into a Streamlit application, " \
    "where you can make real-time predictions, view visualizations, and explore the model's behavior "
    "in a clear and interactive way. Beyond the numbers, I was interested in building a tool that was " \
    "both useful and reliable, and I think I achieved that.")
    
    st.subheader("Classification Report")
    st.text(open("classification_report.txt").read())
    st.markdown("When analyzing the model's performance metrics, " \
    "I was quite satisfied with the results. The model achieved an overall " \
    "accuracy of 80%, indicating that it was correct in 8 out of 10 " \
    "predictions. Looking more closely, for the Not Hate Speech class, " \
    "the model achieved a precision of 0.79, a recall of 0.83, and an " \
    "F1-score of 0.81. This means that it was very good at identifying " \
    "messages that were not hateful (high true negatives), although it s" \
    "ometimes mistook some for hateful messages (false positives). " \
    "For the Hate Speech class, it had a precision of 0.82 and a recall " \
    "of 0.78, showing that it correctly identified many instances of hateful " \
    "speech, but missed some.The macro average (which treats both classes " \
    "equally) and the weighted average (which considers the size of each c" \
    "lass) also hover around 0.80â€“0.81, indicating a good balance without " \
    "overly favoring one class over another. I think it's important that " \
    "the model isn't biased, especially in a task as delicate as this. " \
    "While there's room for further improvement, these results show that " \
    "the model is consistent, balanced, and functional for a realistic " \
    "environment.")

    st.subheader("Confusion Matrix")
    image = Image.open("confusion_matrix.png")
    st.image(image, use_container_width=True)
    st.markdown("Now we can see a bit more clearly the evaluation," \
    " as the model is capable of classificate correctly hate and not hate messages.")


    st.subheader("Error Analysis")
    st.markdown("The reasons why the model can fail is because some of " \
    "the fake positives are cataloged wrong due to the ambuguity of some of the messages" \
    "and the words, like the most used word (bitch), as in the english slang " \
    "it has become a normal word not nessesarily meaning hate, like this case" \
    " others can confuse the model and classify wrong, this also apply for " \
    "sarcastic messages as it can be classified wrong" \
    ". Maybe using a model that can take context and more date can be more precise" \
    " also a different dataset can get better results.")

    st.markdown("This is the end of the project, it really was a pain to do, as I actually" \
    "suck at coding (this is a hate message), However, I did managed to do most of the work" \
    "and got a pretty descent work, Im satisied, I hope you liked, thanks for the class.  ")

    st.markdown("The end :)")
