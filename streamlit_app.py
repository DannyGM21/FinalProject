# -*- coding: utf-8 -*-
"""Untitled1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1gi6lMSaGx-P1wFET-owk44zJbpL6jkUH
"""

import streamlit as st
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from transformers import AutoTokenizer, AutoModelForSequenceClassification
import numpy as np
from sklearn.metrics import confusion_matrix
from wordcloud import WordCloud
import plotly.express as px
from streamlit_option_menu import option_menu
from transformers import AutoTokenizer, AutoModelForSequenceClassification
import torch


# App Layout
st.set_page_config(page_title="Hate Speech Detection", layout="wide")

# Load model and tokenizer
@st.cache_resource
def load_model():
    model = AutoModelForSequenceClassification.from_pretrained("Danny1221Gm12/modelo-hate-speech-binary2")
    tokenizer = AutoTokenizer.from_pretrained("Danny1221Gm12/modelo-hate-speech-binary2")
    return model, tokenizer


model, tokenizer = load_model()

# Load dataset
@st.cache_data
def load_data():
    df = pd.read_csv("labeled_data.csv")
    df['label'] = df['class'].apply(lambda x: 1 if x == 0 else 0)
    return df

df = load_data()

# Prediction function
def predict(text):
    encoded = tokenizer(
        text,
        return_tensors="pt",
        padding=True,
        truncation=True,
        max_length=128
    )

    input_ids = encoded.get("input_ids")
    attention_mask = encoded.get("attention_mask")

    with torch.no_grad():
        outputs = model(input_ids=input_ids, attention_mask=attention_mask)

    # Softmax y clase predicha
    probs = torch.nn.functional.softmax(outputs.logits, dim=1)
    pred_class = torch.argmax(probs, dim=1).item()
    confidence = probs[0][pred_class].item()

    label = "Hate Speech" if pred_class == 1 else "Not Hate Speech"
    return label, confidence

with st.sidebar:
    selected = option_menu("Main Menu", ["Inference", "Dataset Visualization", "Hyperparameter Tuning", "Model Analysis"],
                           icons=['chat', 'bar-chart', 'gear', 'clipboard-data'],
                           menu_icon="cast", default_index=0)

# Page 1 - Inference
if selected == "Inference":
    st.title("Hate Speech Detection")
    text = st.text_area("Enter a sentence:")
    if st.button("Predict"):
        label, confidence = predict(text)
        st.markdown(f"### Prediction: `{label}`")
        st.markdown(f"### Confidence: `{confidence:.2f}`")

# Page 2 - Dataset Visualization
elif selected == "Dataset Visualization":
    st.title("Dataset Visualization")
    st.subheader("Class Distribution")
    counts = df['label'].value_counts()
    st.bar_chart(counts)

    st.subheader("Token Length Distribution")
    df['length'] = df['tweet'].astype(str).apply(lambda x: len(x.split()))
    fig = px.histogram(df, x='length', nbins=30, title='Token Length Histogram')
    st.plotly_chart(fig)

    st.subheader("Word Cloud")
    text = ' '.join(df['tweet'].astype(str))
    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)
    fig, ax = plt.subplots(figsize=(10, 5))
    ax.imshow(wordcloud, interpolation='bilinear')
    ax.axis('off')
    st.pyplot(fig)

# Page 3 - Hyperparameter Tuning
elif selected == "Hyperparameter Tuning":
    st.title("Hyperparameter Tuning")
    st.markdown("""
    - Learning Rate: 2e-5 to 5e-5
    - Batch Size: 16, 32
    - Epochs: 3 (fixed for time constraints)
    - Dropout: 0.1 - 0.3
    
    Best configuration:
    - Learning rate: 3e-5
    - Batch size: 32
    - Dropout: 0.2
    """)
    st.image("optuna_trials_plot.png", caption="Performance over trials", use_column_width=True)

# Page 4 - Model Analysis
elif selected == "Model Analysis":
    st.title("Model Evaluation and Justification")
    st.markdown("""
    - **Model**: DistilBERT fine-tuned for binary classification.
    - **Reason**: Efficient and smaller transformer, suitable for limited compute.
    - **Challenges**: Imbalanced data, informal language, short texts.
    """)

    st.subheader("Classification Report")
    st.text(open("classification_report.txt").read())

    st.subheader("Confusion Matrix")
    
    fig = plt.figure()
    cm = np.load("confusion_matrix.npy", allow_pickle=True)
    cm = cm[0] 
    fig, ax = plt.subplots()
    sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", cbar=False, xticklabels=["No Hate", "Hate"], yticklabels=["No Hate", "Hate"], ax=ax)
    ax.set_xlabel("Predicted Labels")
    ax.set_ylabel("True Labels")
    ax.set_title("Confusion Matrix")
    st.pyplot(fig)

    st.subheader("Error Analysis")
    st.markdown("""
    - Some false positives were sarcastic or ambiguous.
    - False negatives included subtle hate phrases.
    - Improvement ideas:
        - Use of context-aware models.
        - Better annotations or data augmentation.
        - Ensemble approaches.
    """)
